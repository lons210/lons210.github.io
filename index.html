Umm, I actually thought about this in anticipation of our conversation and my understanding, which is not as in depth as I feel like it probably should be. 
Is that sort of regular computing processes were data in data out they they knew how to do one thing and AI is more. 
Sort of brings into it a humanistic approach to be very, you know, humans are the center of the world, that it does. 
In addition to that, input in and input out, there's. 
There's coding that allows for like nonlinear correlations, right? 
They can take huge data sets and and sort of learn from that and make for lack of a better word, like intuitively taps and which is is more how the human brain works. 
 
Well, I think you know actually that is a hugely problematic area in that. 
Yeah, artificial intelligence and its use in all industries, in the law and medicine everywhere has sort of outstripped, uh, the the regulation and the ethical considerations. 
And I think when you have ethics and any field, what you're really doing is taking a a multi level approach where you look far ahead to the ramifications of of any action as well as you look back to to make the the correct choices. 
And I think it's very important to make it difference between morals and ethics. 
Umm, ethics are are very much. 
There to protect everyone. 
Sort of. 
And morals is more of a a individual guide. 

 
Generally, or in healthcare or specifically, how do you want that answered? 

 
So ask me the question again, sorry. 

 
So the most beneficial part is I think it can allow if you properly people, to offload some of their workload and the things that they might have to spend time. Umm. 
Sort of formulating responses, for example or that kind of thing. 
They can offload that workload to to certain programs like Chat P and or GPT and Bard and those kind of things and and you can also use that to. 
Enhance the work you're doing right? 
So like if you if you always are writing a typical kind of letter and you want to either jazz it up or change it up or have or think, well, what else maybe I might not putting into this you can you can run it through a program like that and and get more suggestions. 
I think the detriment is that it is. 
If if it is improperly used or used by somebody who does not understand the limitations of AI. 

 
It can really produce a false narrative and you know, they they talk about like AI hallucinations and and I think everybody, not everybody, but a lot of people have put in, you know, their their name or their what they do or something and and some of the things that get spit out by AI in the more commonly used ones. 
It's just nonsensical, and so you have to have a real, uh mechanism in place to ensure that you're not taking information is true. 
That's not true. 
And and I think that that is a huge detriment that is on both the side of the AI and on the side of the ARAI user. 

 
So what type of AI do you currently use in your organization? 
Well, well, so I I am not patient forward. 
I I don't deal with patients directly. 
I do know that there is AI in use in diagnostics and in in some of the umm, you know, what do you call it like? 
Video conferencing meetings. 
I do. 
I also know that there's a I used in uh, like drug development, but I don't. 
I don't really know that much about that as far as as what I use it for is. 
The more a sort of generic AI of I have utilized chat, GPT and I've utilized Bard, which interestingly I have used both on the same things to see what sort of differences you get and and it was. 
It was remarkably different. 
The information that you got out both positive and negative for both. 

 
I I definitely do. 
I think that as far as what is about to start and go forward is there's a lot of talk about AI and sort of shoring up accountability. 
Who is it that is accountable if AI is utilized, you know by the actual medical providers more so than than you know what? 
What goes on in the back office? 
But you know there's real concerns with the potentiality of breach of confidential information with the use of AI. 
There is, like I said, that the real concern of uh. 
False information. 
False narratives getting out there because of the use of AI and and I think that is begging the question of so who is accountable? 
Is that gonna be the? 
Umm, you know, sort of IT that installs this and and and does the programs or is it the users? 
Because I think there's a real issue with, umm, the users getting complacent with what they use and not understanding it well enough and you know they use the 110th part of the AI that that is beneficial to their practice or whatever they're doing and they sort of neglect the rest. 
And they neglect to learn the implications of what they're doing and and so they should have some responsibility, right? 
It's just, it's just like anything a bad data goes in because of human error. 
Also, though with AI you have the added layer of, it's just not bad data being put in. 
It's bad data then being extrapolated and then interpreted. 

 
I think that, you know, I've just started giving this thought and I think that there needs to be in depth education of the AI users and bull to take a step back when when the administration decides to. 
Put AI into things like I said, like drug studies or patient care or or even back office stuff. 
There needs to be someone who is a countable, whether that be the IT department, whether that be who ultimately signs off on the decision to use, which program versus whichever program. 
They can't just say, well, the salesman told me this was a good one. 
Right, they need to have some level of understanding that of what they're purchasing. 
Then, once AI is put into place, I think there's the next level of accountability, which has to lie with the user. 
And it is nobody is going to expect a doctor to understand the intricacies of how AI works right and and how the programming was and and how the AI learning actually happens. 
Nobody's going to expect that, but I think it is not unfair to say they do need to understand to a level which makes their using of it competent. 
So I think that there is an accountability for competence for all the users, and I think that goes down to the most minor level that there has to be understanding because it can be very. 
Alluring to use and easy to use. 
And you sort of forget the dangers of the pitfalls of it. 
So the actual users need to be accountable as well. 
Ohh. 
Right. 

 
And I think there is an A, A level there could be and this would need more consideration. 
There is a level of accountability that is put upon like the patients that they should, umm, if if AI is being used in their care, they should be made aware of that and acknowledge that it's happening and either say yeah I'm good with this or I'm good with it. 
If you know it's run by me or whatever, I see the results or no, and I think that that, that level of accountability can't be ignored. 

 
I do. 
I think that I have been very reticent to use it too much because the areas in which I would want to use it are, well, there's some areas in which I use it pretty freely, like like judging up a letter or. 
Or changing the wording of something you know, that kind of thing. 
But when I want to use it sometimes is in areas that I am not. 
Familiar if it's an area that I'm not as familiar with. 
And uh and and so you can ask the eye questions and it gives you answers. 
Well, if I'm not as familiar with the area, I don't know if those answers are on point or not. 
So I I have been very reticent to use it unless I am, you know, 99% comfortable with the area and no a lot. 
So that I can see if something suddenly doesn't make sense, or if and and I can give you an example that there was A at a question with regard to a specific area of law and the standard of care in that area, and it answered with the federal uh standard of care versus the state. 
And what I needed was the state, and if I didn't know any better, I would have thought. 
Oh well, that's what it is. 
But I but I happen to know and I do also happen to know that it's different in all the state, not all the states, but but it varies widely within the states. 
And so it didn't even seem to comprehend that I was asking about my specific state. 
It it just gave this general big answer and it was wrong. 

 
 
Oh, I think I umm, I don't know how to put the to put it somewhat. 
With umm to. 
I don't know how to say this. 
Uh, there is a wide variety of levels of sophistication in employees. 
In our system, you know and so I would be very concerned about summon employees versus others. 
And I would think before allowing a on any kind of widespread use that I would, I would want mandated training and understanding because there are biases that can go in right. 
And asking the questions to the AI that come from the socioeconomic, the racial, the, the, the sexual makeup of that individual. 
That could really skew the result, and I think that that there would need to be pretty intensive training before I think it would be affected to have widespread use 

 
Ohm, you know, I think unfortunately the bigger the organization, the slower the role is on those kind of things and you know I know that when we instituted and a diversity and and and inclusive training you know it it was a fruit of an idea and then it took six months to run it through to find somebody who could actually roll that out. 
And then it took another six months to get it rolled out. 
And then another year or so to get everybody trained. 
So I think that that it is in the works to talk about it and the importance of it, but we're we're nowhere near actually implementing it. 
Uh administration wide. 

 
Umm, I think with regard to patient care, you know there is. 
You know, Healthcare is is unique in that mistakes are literally life changing and I I I think it would need to be, Be, umm, somewhat silo not siloed. That's not the word I'm looking for it that it would need to be on a team, for instance that was that was dealing with patient care, that it would need to be, need to come from the top down that the primary physician in charge of that patients care. 
You know, like, like the medical oncologist is primarily using the AI and authorizing use of it in diagnosis and and and Sort of management, you know, patient management. 
Authorizing that down right from to the surgical oncologist to the nurses to the you know, whomever the palliative care team, you know, all of that, it would need to, it would need to come from one source and then be, umm, sort of overseeing the whole way down that, that process. 
And I think there's some jobs that AI is just not. 
You know, there's no place for it, right? 
There's no there's no real use for it. 
There is. 
There are ways things happen that you know having AI is gonna do absolutely nothing to change the way those things happen. 

 
Uh-huh. 
Well, umm, I think that calls into question the very validity of the AI, right? 
If if they if you are being asked to. 
Utilize a program and yet are being given no umm, detail or uh? 
I don't know. 
You you get nothing to show you that this is a real thing, right? 
Because then it could be. 
It could be the magic beans. 
You know Jack gets magic beans and is expected to assume they work, and in his case they did. 
But, umm, you know what? 
If those magic beans were supposed to just actually grow beans, and instead he got this giant Beanstalk and I think that that is, that is the danger of AI, right? 
Is everybody is using it thinking with my bias that I'm getting. 
I'm getting actual beans that are gonna grow. 
You know, crops and instead I'm getting something else and I can't go to the source to find out what it is. 

nd I think that that is is problematic and would need to be. 
Be uh. 
You know that goes back to the accountability, right? 
The accountability just can't lie with the purchaser. 
The main user it has to lie also with the provider. 

 
