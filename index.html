Sure. 

How do I define it? 

 
I guess the way I think about it is the the AI's based off of a training set of information and that training set of information can put a lot of bias into the situation if not done appropriately. 

So if you're talking about anything AI that you know could cross different lines, whether it's first of all just data integrity and sharing people's information that maybe you shouldn't share, or if you start getting into bias or discrimination of any legal standing, that's what I kind of think of, you know, AI ethics, but it comes back to that training set and, you know, making sure you're you're understanding that that information in regards to different things like gender, race, etcetera come or if. 

I if if you're using something as a decision making tool based on a training set, maybe it comes to a overtime standard pattern of answers. 

Is that standard pattern biased or discriminatory in any uh aspect or or way of of life that we're, you know, the normal topics we think of set clear and clear I think. 

Yes. 

Yeah, let's say so. 

Most beneficial health care. 

I see my world is diagnostic imaging right? 

So to to me, it's incredibly beneficial because there's so many different variables. 

When we try to create images and if you can train software Recon algorithms you know to to understand what those are and can and continually make them better. 

So you have a better image for better diagnosis from a doctor like that's I think that's fantastic, right. 

Uh, noise reduction in CT images to make sure the anatomy is super clear. 

So you can see smaller and smaller anatomy for the doctor to make again that diagnosis or that radiologist to make that diagnosis, I think is I think it's great. 

It's incredible. 
That's why I agree with it. 

In the world that I live in, umm, detrimental, I guess maybe there's more fear than actually knowing where it could be detrimental, but I think again, I think it comes back to that bias, right? 

So whatever. 

Whatever you're trying to design in the AI space again, it's going to be fed by information. 

So how are you? 

How are you defining that data set or that information that you're going to use to train the algorithm to do its thing? 

Umm, you know, statistics have to be of utmost important. 

How many? 

You know how many pieces of data do you have to have to be accurate to not introduce too much bias or the wrong bias for the situation, et cetera. 

So detrimental. 
I mean it, data proliferation could maybe be detrimental, especially if people don't want their data out there. 

I think every time we find something that can be used for good, it can be used for bad. 

So you know our people able to steal identities, you know, does it allow certain groups of people to discriminate in a way that appears to be legal, yet it's done for their benefit or something. 

So, umm, you know, the more sophisticated the technique the the more sophisticated the criminal. 

I guess that's the way I think of it. 

Almost. 

Yeah. 

Well, yeah. 

They they gotta be right? 

So I guess umm it's super powerful, but it's all based on the data set that you're going after and again up front that training data that you're going to define, that's where the ethics in my mind really matter. 

It's not the algorithm like it's not the neural network. 

It's not the that stuff. 
It's up front. 

I've got to produce thousand images that represent a a kidney ailment or something. 

You know whatever. 

Where do I get them from? You know? 

Is it 50 males and 20 females? 
Is it 5? 

You know African Americans and 300 Caucasian, you know Americans, right? 

So like, where do you get that information? 

How does that play into the situation? 
That's a example and and in our scenario, but I can't even imagine what other aspects of healthcare have to think about when it comes to all those different biases. So that's to me, the the hardest and most should be the most debated portion of what we're doing here. 

Not right now. 

I mean everything. 

Well, I should say on our current piece of equipment, that's what's released or it's basically a noise reduction algorithm and that's because we've taken some of the best images we've ever produced and you sit down with radiologists and you confirm that right? 

And then that's what's kind of feeding that noise reduction algorithm. 

There are things that are in development around being able to. 

I have like identify anatomy within the body. 

So then you can do organ segregation in your like in your image and your UI. 

So you can navigate the UI and find certain things that can tell you boundaries, borders, set up scout areas or scan areas so you can you can have less dose etcetera 
But everything is like positioning like we we do use a deep learning for our, we call it express mode patient positioning. 

So I can identify, you know, whether your heads at the top of the scanner or your feet at the bottom, etcetera and make sure it's setting up the scan right. 

But the thing we're doing is positioning and noise reduction. 

So I would say like kind of basic maybe in the concern around ethics for AI. 

Not really, no. 

I don't think so. 

They not that I'm aware of anyways, unless somebody tells me he's fancy tools. 
They they give me to use has it in there, but I'm not aware of it. 

No from like a umm, like a corporate like franchise level software that they let us use. 

Now I'm I'm not necessarily developing product, I'm more managed product, so all the fun tools I use. 

Here are just databases like Excel and whatnot, so not so much in my personal life. 

What's? 
What's that? 

Yeah, yeah, yeah, absolutely. 
In my personal life, not really. 

Unless, like social media has it built in, I'm not sure. 

I don't think so. 

I'm not like purposely doing it and everybody talks about chat, GPT, a little bit, you know, but I don't use it right now. 

No, just there hasn't been a there hasn't been, like, an overwhelming reason for me to use it in my like day-to-day function. 

I have coworkers that are like, yeah, you should go try it. 
It's fun. 

It's neat for an answer or information. 

Sure, everything I'm tackling right now, I don't need it, but it it's not like I don't want to. 

It's just I haven't had an overwhelming need to go give it a shot at the moment, but I'm pretty open to it. 

I just haven't used it yet, so that's about it. 

Do I have evaluate them not like as part of my job? 

No, I mean we so like I I run global operations for engineering to to team that's dealing with our deep learning algorithms et cetera as our IQ and Recon team. 

So if if there is, if there's any group that's really like having to think about those ethical concerns, it's gonna be most likely the the IQ data scientist side of our Roy Nelson's team, the IQ team. 

Umm, no, that's OK. 

No, I'm pretty sure. 

I'm pretty sure they're already are a few different standards you have to adhere to, and like our our a group, I has to has to be able to answer those. 

Especially when we're turning in this new application for FDA approval, they want the proof. 

They want to understand exactly how you're doing it that's been driving. 
I think most of our internal practices is the fact that you submit for 510K with the FDA. 

They wanna. 

They wanna understand that it's safe and effective. 

It does what it says it's supposed to do, right? 

So this is where you're putting your approach. 

You're putting your statistics. 

You're putting the design out there for them to understand what's going on that drives most of the practices by a regulatory team and our internal work instructions. 

So it's is, it is like I don't typically have to use those work constructions. 

Those standards, the regulatory standards like they do does it, does it drive the conversations between regulatory, that IQ group, absolutely, we have a whole regulatory group that looks at submission to the FDA etcetera. 

I mean, we have to be comfortable with what we submit, right? 

And if there's any questions you're required to answer them all within a certain period of time. 

So yes, we we do. 

I'm not typically part of those conversations. 

I'm hoping Roy is doing his job. 

And Helen already folks. Yep. 

Yeah. 

You know, so if you're specifically talking to a I I'm not aware of all the differences country to country. 

But yes, we have to abide by the regulations that are published in every major market that we're selling in, right. 

So we have the the big ones are CAFTA or MPA and FDA. 

But there's there's so many of them, right? 
We're we monitor compliance change notices which are you know our regulatory warnings that new things are coming out. 

And yeah, if you wanna. 

If you wanna keep a mark on your product and being able to import and sell in that market or that country, then you have to show here once to the regulations. 

So they're all different. 

They're all different and we have like for instance, our organization as a point person that monitors all cease CNN's eye track. 

Track how many of them are staffed. 

Uh, and how many of them aren't and could shut our production down? 

 
 
Can't ship, but yeah, it's a constant battle, regardless of AI, the regulations around the world are constantly changing. 

 
With what we do, it's just about the data stuff like so. 

 
Umm, you know, we make we make images, we make really pretty pictures for radiologists to use and we often wanna test applications or development, right. 

 
You know, our apps are in process to to get her hands on raw data from patients. 

 
Images is great, but there's you know this HIPAA laws etcetera. 

 
So my only concern with like the employees using AI tools is is more of the data proliferation, the HIPAA rules that we might break because again, we're using, we're trying to use patient data in certain cases. 

 
If all sudden you know you're using some tool that I don't know, it's creating an algorithm for you based on input data and you're starting to drop your own data in because you're trying to create something with cool. 

 
Do fancy application? 

 
Does it just put that data out on the web now? 

 
I mean, be careful, right? 

 
It's not our information and we typically you know we have to shepherd that data like it's still sensitive and has all the patient information. 

 
So you have to be careful with it. 

 
Otherwise, I'm not. 

 
I guess not too worried about it. 

 
It's it's more of the whatever we sell that's going to be DL AI again. 

 
What's the function? 

 
Where is the data coming from? 

 
How do you pick the training data? 

 
Umm, you know statistically how many pieces of information do you have to have and from where to make it accurate and not overly biased one way or the other. 

 
But using the tools I don't, I don't have any issue with it. 

 
If they want to just don't let our let our sensitive data get out there. 

 
Yeah. 

 
Just nowhere know what it's doing. 

 
Know where it came from and be careful if it's, you know, some kind of solution or system that allows you to input data. 

 
What data are you putting in there? 

 
Because, I mean, I, regardless of HIPAA laws and rules that we have to deal with, I mean every country has their own. 

 
The other one is just IP stuff, right? 

 
I mean, you like one of the one of our biggest markets I think is also the biggest. 

 
You know cause of IP leaks, right? 

 
And so we've had and we've had lots of data leaks here and in Waukesha, go to other countries, we didn't want it to go to right. 

 
I mean, we're very strict on our own IP and our own systems and locking them up in security and whatnot. 

 
So that's the other one too. 

 
It's just you can't let the IP go all over the place. 

 
That's our, that's our livelihood going forward, right. 

 
So that's that. 

 
Otherwise, no, I mean, we're normally, so we're normally pushing the engineers to like, yeah, think creatively use whatever you need to to do your work. 

 
And if there's something cool and new that makes sense, you know, and it can be innovative. 

 
Great. 

 
Just, you know, don't don't let the IP and and confidential data go. 

 
As long. 

 
Yeah, did the same comment, sure, but where did the data do you understand where the data came from? 

 
Like if somebody came, if somebody came to me and said AI gave me this decision to do this, I'll be like it's asked the same question. 

 
What was this training set? 

 
Why do you think this is valid? 

 
Like, why is this information valid for this use, especially if it's something that you know for us has to go through trials and it could affect the standard of care. 

 
Uh, you know, certain things lower level behind that don't produce an image, but they help the system operate. 

 
If it's giving you, you know information or a decision on, hey, I wanna. 

 
I wanna do this like packet data transfer method because I got this this answer from AI. 

 
OK, fine. 

 
Give it a shot, but if it's like, well, I'm gonna do this particular filter on the bone to brain interface because it's supposed to do this. 

 
For whatever I'd be like validated ballot, show me that it's actually doing what you think it's going to do versus before. 

 
So even if it makes a decision so to speak, we still have to verify and validate it. 

 
So it's like, sure, but you better validate you can't just say, Oh well, this told me to do it. 

 
Therefore, it's OK that's not how this works. 

 
So that's the way I think about it. 

 
Well, I mean, every every product rights and our well, we have our process, our own process, right. 

 
And then we have this historic S on what it takes to make images for a CT scanner. 

 
What it takes to make sure we have safe and effective product. 

 
So yes, there's I'd say a very standard approach. 

 
Expectation for verification, validation, reliability, et cetera that has to be met. 

 
So yeah, I don't know if you have more specific questions about it, but yeah, I mean, we have a laundry list of requirements that we have to validate. 

 
So I think like development ethics and like how well you lock it up, umm. 

 
Or like I think so first of all, yeah, I think the responsibility to be ethical upfront is you know, the responsibility of who's designing it. 

 
Like you're releasing something so you know what you're doing. 

 
The training set you use should be based on that. 

 
May ethical view of the situation with the right data inputted for you know whether it's demographics that you plan to use or whatever. 

 
It should be not biased by the companies agenda. 

 
It should be reflecting the world as as we see it. 

 
So I think yeah, to it's responsible if the development center I think where it gets super grey is like the whole cyber security side of life. 

 
I mean, it's great. 

 
Today, the hospitals want to hold you accountable. 

 
With, like GE accountable for any cybersecurity issue. 

 
So you know, then you have standards that are put out there by like, you know, DoD, right? 

 
They've got these stigs they run to make sure, umm, you know you you can pass them, right? 

 
You're not vulnerable, but new things come up every single day. 

 
So to me, that's the ambiguous Gray area that, like, is as long as there is like a baseline to say, yeah, I'm not vulnerable to this known list of issues. 

 
Then it's easy to say the manufacturer, the developer, is responsible for cyber security vulnerabilities, but it moves so fast it changes so quick. 

 
There's always prerequisites to keep things safe on a network, et cetera. 

 
Being very specific, there helps you understand who's responsible, because at some point we make the device we hand it off to somebody. 

 
If they modify the device, if they don't actually connect to appropriately. 

 
If they don't keep it protected appropriately, we can't be responsible at everything they like to hold us responsible to everything. 

 
But there are requirements in an install manual, et cetera, to keep yourself from being vulnerable. 

 
So that one tough. 

 
It's always gonna be tough because there's a million variables out there and they're changing constantly. 

 
Those are the two I think of development and then like the cybersecurity side of it, how a hospital decides to make decisions with with whatever is presented to him again, like or giving you this answer based on this data, it's been validated. 

 
Here are the vulnerabilities we know are good too. 

 
Beyond that, you know, it's kind of, you know, but I think it should be on them. 

 
But history would show us it's also been on us before, so that's that's so hard. 

 
Yeah, I don't know if you remember like 2008. 

 
There was a like there was a there was a hospital in California that overdosed. 

 
A a kid, his hair fell out. 

 
He was overdosed so bad by a CT scanner. 

 
And, umm, we weren't at fault because it was. 

 
It was misuse of the scanner. 

 
At the same time, we were making changes to the scanner so that could never happen again, and then the scrutiny on the scanner manufacturers. 

 
Since that time has gone through the roof because people are like, wow, you know, awareness that this is possible. 

 
So it's kind of like we weren't responsible, but we are. 

 
So we're trying to do something and make it better. 

 
So there's like this. 

 
It lands on them, but this joint responsibility to make sure it can't happen again kind of thing. 

 
Yeah. 

 
I mean, once it's foreseeable misuse, I think we have a responsibility to to, you know, design and control. 

 
So it can't happen. 

 
So it's like again we we can't control everything a customer is going to do with our scanner or our AI tech. 

 
But if it's foreseeable misuse, then we should probably have a mitigation for it. 

 
There is a process. 

 
Yeah. 

 
I mean, at least. 

 
What? 

 
We try to do. 

 
I mean your you assess your design for risk and failure. Umm. 

 
It's it's uh, it's an iterative process, right? 

 
You hope you have a group of of bright individuals that know the design and can think through the ways that it could fail. 

 
So you can define that foreseeable misuse and then try to mitigate it out. 

 
Mean FM. 

 
EA's are used. 

 
We have a kind of a risk assessment process and document that happens for our scanners. 

 
That's constantly update. 
I think that's required by the FDA as well required for the life of the product. 

 
So you're hopefully learning overtime and updating it. 

 
So yeah, I mean AI or whether it's what software. It's hardware. 

 
Yeah. 

 
I mean, you still go through the same same thought process. 

 
You try to understand how it could fail in CT development or I would say at least for sure, CT development feels like 40% of our code is actually umm, like fault handling fault mitigation. 

 
So bad things can't happen. 

 
Yes, you have ionizing radiation and you have a giant mass that rotates around a patient and high voltage and whatnot. 

 
So, umm, you think about it all the time. 

 
You design for it, so I think this is no different, but that you can't design out ethical misjudgment of a person. 

 
But hopefully you're not designing it in from the start with your training set. 

 
Umm. 

 
Shawn. 

 
No, no, I I think well, I mean I'll I think I know, I know what you're asking. 

 
I'll answer and I'll see if it if it answers the way our like our internal regulations or standards, work instructions quality system is is a. 

 
It it's a conglomeration of like ISO, the you know 21 CFR, part 11, subchapter J, like like the little Green book from the FDA, plus all of our our audits, plus all of our issues that happen in the field, customer complaints, safety issues, et cetera. 

 
I it's it's. 

 
It's an ever changing, ever evolving quality management system. 

 
Umm, so is it. 

 
Is it more than the government gives us? 

 
Yeah. 

 
It's like, what, 20 governments plus like our own trips down the road or speed bumps down the road. 

 
However, you wanna say it have have caused us to change, so it's a. 

 
It's a conglomeration of all of that does it? 

 
Does that answer your question? 

 
Not that I'm aware of, unless they're doing something new. 

 
No, I mean it's it's pretty standard. 

 
They filter through every single that complain. 

 
Every single complaint that comes in manually, they pray to oak the biggest buckets. 

 
We dig into everything that comes up. 

 
Umm they they will assign categories of complaints. 

 
Then they'll they'll look for trends. 

 
Then they'll dig a little bit deeper, but if it's a hazardous complaint or investigation, it's it's immediately looked into. 

 
But I don't think there's any AI in that space whatsoever. 

 
We aren't like our group isn't, but the larger health care. 

 
Ohm is talking about. 

 
It is talking about data, the importance of data and the awareness of of what it could be. 

 
Yes, that's coming through from the regulatory side, but like myself and MCT engineering, no, I'm not, no, we not that I know of. 

 
Yeah, there's a whole. 

 
There's a RA executive that sits on. 

 
Peter Tweenies staff since the the president of the company, they have management teams all over all different SKU's. 

 
'S so I'm an MIT that's molecular imaging. 

 
If you have time myography SBU, there is Mr there's Women's Health, X-ray, there's a whole bunch of them. 

 
Them every single one of those has an area manager with RA, umm engineers. 

 
So like for instance, we have a PR manager and three RA engineers that are specifically assigned to MIT Engineering globally. 

 
So they're they're embedded to us. 

 
So we have a whole group plus they're embedded within the business units. 

 
That's that's regulatory. 

 
There's also the quality assurance group too, which has the same scenario. 

 
So yeah, we have both fact the offices right over here near mine, Glenn. 

 
So most of the the regulations of trying to think like so many of them. 

 
I mean, most of them have the same theme, right? 

 
It's it's a define what you're going to do or or define how you're going to handle or manage this situation and then do it right, documented appropriately, do it and show it show with proof that you can handle it. 

 
How does it change what we do or how we're developing uhm. 

 
Most of it, like the cyber security stuff I think has changed a lot in the software space as of recent how you're supposed to develop, it's come up a lot in the last few years. 

 
So you're thinking about vulnerabilities. 

 
I think you know more at the unit level way upstream. 

 
I think from a like a mechanical engineering says well, a system design in general. 

 
I think it's reliability vulnerabilities. Umm. 

 
Writing more requirements for these things up front. 

 
So you're you're literally starting from Ground Zero with those things in mind. 

 
So if anything, it's it's it has made it more difficult, umm to understand all the requirements and and put them in. 

 
It just makes everything more complicated to develop and design, but I don't think any of it is like fundamentally changed what we're trying to do. 

 
I mean, we're still, we're still trying to make a really good image and our bigger challenge is the fact that we're constantly wanting to defy the law of physics more so than their regulations. 

 
But it's it's like, yeah, how do you take certain things into consideration upfront? 

 
Like, how do you design for EMC for instance, in the hardware electrical space? 

 
How do you design for vulnerabilities in the DoD and software space upfront now? 

 
It would be what kind of conversations do you have to have and how do you gather data sets for AI to make sure you're comfortable with the training set that you're going to use? 

 
Umm, but I don't know if it really fundamentally has changed what we're trying to do, so I don't know. 

 
Hopefully that answers your question. 

 
I did like 100 engineers tomorrow, so yeah, I'd be OK with it as long as you can verify and validate its effectiveness. 

 
Yeah, absolutely. 

 
Like he's, I mean, there are some software people here that, I mean, probably have debated this topic 100 times more than I have. 

 
I'm sure is debated this many many times, so yeah, just checking my background's also more hardware than it is software, so you got the you got the high level like the business operating guys view of AI. 
