Ohh, even internally externally people talk about AI. 
 
They they think of AI in terms of all these generative AI tools and PT and a lot of folks are not aware of the the vast other areas that deep learning machine learning can can help and then they are using on a day to day basis without without knowledge actually. 

 
So alright. 

 
How would I define it? Ohhh. 
Interesting question. 
I didn't really haven't really put a lot of thoughts into what the definition should be. 
What? 
I'm definitely aware of some of the concerns, whether it's healthcare or. 
Uh beyond healthcare. 
Like with it's automated. 
Driving have you have you read this book? 
The 21 lessons for the 21st century by you all know Harare, uh so so this is a sequel of the book. 

 
Probably you have heard the book. 
Uh. 
Sapience. 
So this is the kind of a third book that he wrote on the same topic. 
So that's mostly thinking about what Karen Technologies are and then how would that change the future and the way that society is think for example, like if you take driving, for example, right, if you go to automated driving some of the technological problems that we face today. 
So, so we talk about when we talk about AI, mostly the technology side of it, but some of the unconscious, subconscious decisions that we're making when we lead it, when we let the program do it right, there's a lot of impact. 
And if the if you have to feed the algorithm, you had to think how to feed some of these philosophical questions right? 
Like if you have a risk of, let's say automated vehicle goes and uh, it's predict that it's going to get into an accident and the options are you either put the passenger in the hands say the driver in the harm's way or the pedestrian in the harms way. Right. 
So how do you decide how to write the algorithm right? 
And so obviously this is an ethical question. 
Philosophical question, right? 
So this is applicable at different levels, right? 
In healthcare? 
Uh, for example, is it? 
How do you deal with the patient privacy? 
Right, so. 
I I don't know. 
Probably you're you're aware of this. 
The things that we do to diagnose, do to, to identify certain diagnosis with the with our images. 
So we run certain deep learning. 
Algorithms to figure out what kind of anomaly these these patient may have right now, sharing this with the patient or the healthcare provider that now this is an ethical problem, right? 
So is it the right time to share this? 
Is it the? 
What if there's an error in the calculation? 
What if it was misinterpreted? 
So all these are kind of philosophical or ethical questions that I don't think we have put a lot of thought into this. 
So to answer your question, I I had to think a little bit to come up with the definition. 

 
Uh, but the there's a lot of concerns. 
I don't think that we have done enough. 
Umm, we haven't put enough thought into how to address these because you have been more involved in the technological side of is capable or not, not necessarily. 
What's the impact of it not? 

 
Ohm's most beneficial. 
Uh, obviously the advantage is that comes with it, right? 
So it's a lot of data driven decision making, uh operational efficiency. 
How do we like, do data driven like inventory controls, right. 
Uh, optimizing resource allocations? 
Talent management, right? 
How we find right people for our teams, a lot of predictive analysis. 
What else? 
A lot of risk management that we do right? 
Uh fiber security risk assessment. 
We can find out. 
Look at the trends and and see what should be really working on a lot of continuous improvements, learning related stuff, automation of repetitive tasks, right? 
So there's a there's a lot of benefits. 
What's the opposite that you asked? 

 
Looks detrimental aspects would be. 
Uh, well, depends on how you use it. 
Right. 
And and I see a lot of. 
I mean, we go to. 
Uh, yeah. 
Radiological Society of North America, for example. 
It's a lot of, uh, the uncertainty unknown of where the technology is going, that's creating a lot of anxiety among a lot of people, right? 
Would this make lose my like job? 
Or would the radiologist job will go away if the machines can detect the read the the MRI or CT for example, right? 
If and the biases that we have in a, I would they work against us, right? 
And so those are they're not. 
I mean, I don't see there's concrete evidence, but the unknowns or the potential is where I think where most of the detrimental impact. 
Uh, so as I said again, it's based on how we use. 
So if you use it for the right thing, you can kind of. 
Make sure that doesn't happen, but if you without awareness, if you just blindly use it, you might be impacted by this, right? 

 
Uh, yeah, in in many, many areas. 

 
So I mean what we do is mostly medical device manufacturing, right and healthcare. 
So even ohm even to create images using these machines to reduce noise, trying to figure out how to write algorithms, we have a lot of algorithms that I based so just to to get images and also once we have images how to read those images, how to diagnose certain. 
Abnormalities. 
Do people have anymore thorax? 
Do people have TB right? 

 
Or if you are feeding a baby, are we putting the tip into the right place? 
Are we fit in the lungs? 
So there's a lot of areas that we use AI to prevent mistakes that happens in the in the field. 

 
Uh, it's mostly on the technology and product, but yes, for organizational also we use it like uh, I I I'm not sure if you're doing it knowingly or as a byproduct of using some of the standard tools, right. 

 
For example, if you use LinkedIn for to to do searches right, LinkedIn heavily depends on AI to narrow down the searches, find talent. 
So we may be A and also we use other tools to do certain like HR tools to do certain analysis, find out things uh, to to basically do data driven decision making. 
So so these are developed by external companies that are available commercially. 
So as a user of those, we are either we like it or not. 
We are subjected to the the biases and. 

 
But not that they have created for us. 
Well, it's. 
It's without really knowing how they were implemented. 
It's not easy to because we don't have enough data points to see how those biases affects us, right? 
Like when I because you need a larger sample size to see how it affects. 
So we don't have like I don't have access to a broader data set to see how that would affect, right. 
So there is a lot of work that is going on. 
For example, after this chat GPT came up, so a lot of people are using it right now. 
How are so we are trying to put guidelines to see how we should safely use it. 
And how can we not be subject to unknown biases? 
So there's there's work going on. 
Teams are working on really analysis because it's still new. 
We don't even know how people are using this, so we are trying to find out how teams individuals are using these resources to see what are the implications. 
How what should we do to be prepared properly and to ensure that we accidentally don't misuse so. 

 
Ohh. 
Subject to uh devices that we don't want to. 

 
Yeah. 
So if you are analyzing an image, uh, it's there's no to my knowledge there's no ethical factor to it because you're it's a bunch of pixels from right. 
But you can tell the confidence because based on how the model is trained, you can tell it, OK, there's a 96% chance of this image showing signs of pneumothorax, for example. 
Now, ethics comes the moment you start showing this to whom? 

 
Because now you don't know to whom you are showing this right at what time you are showing this. 

 
What if you are showing this to the operator and the operator immediately shares this with the patient's, right? 

 
Is it now US actually has certain guidelines against these like to prevent or to minimize the damage. 
Uh. 
Potential damage that could be caused by sharing these prematurely before a radiologist actually reveals this. 
So uh, but Europe doesn't have those. 
So for example, if the if our extreme machine detects a image with a certain anomaly in the US, we don't want they. 
They make sure that we don't share that information with the operator until 15 minutes elapse. 

 
OK, so even still we can we have restrictions of what we can show we can't even show that there's a 90% chance of this person having this. 

 
Instead, we can say this is flagged to be prioritized by a radiologist, right? 
So now human actually looks at it and then makes the decision and agrees with the I findings and then shares it with the the provider or the patient rather than directly. 

 
Ohh let the machine share this because we don't have control when the machine shares this information, right? OK. 

 
That's a good question. 
Unfortunately, I'm not, so the the work that I'm doing is mostly on the device side, right? 

 
So in the past, teams that I have been so I have not been participate because I do all the back end stuff, which is mostly the technical stuff. 

 
But how that is integrated into the patient workflow? 
Uh, that's beyond the scope that I'm aware of, so I don't know if they're working on it or not. 

 
Employee use of I uh. 
I actually don't have much concerns. 

 
I I think I mean at least the team that I have is our professionals who are knowledgeable about computer engineering, computer science technology very well. 
Uh so far. 
I I don't haven't seen any reason. 
Obviously they're like proprietary things are there where people could use, like in chats, GPD. 
If you put our code and say hey, what is this doing? 
That's the problem, but those are more like they're not ethical problems. 
They're more company trade secret kind of stuff. 
Apart from those, your question is more like and. 
Can the team use it in a detrimental way? 
Right. 
So that do I have concerns is that. 

 
Ohh. 
Give me an example like what kind of decision making? 

 
Yeah. 
Uh, so for the first example that you mentioned, uh. 
So that's beyond the scope of what we are doing. 
So we are mostly on the back end as I mentioned. 
So that kind of things is beyond the scope of our team. 
So I don't have any worry about our team doing anything like that now. 
Something like chat, GPT. 
See if somebody uses the deputy and make a mistake. 
Am I worried about it? 
No, because they could make the same mistake even if they do it themselves, right? 
So the the concern is how do we make sure that we have checks and balances. 
Uh on the decisions, regardless of how. 
Ohh, who made those decisions? 
So that's where I'm more concerned about do we have the right checks and balances in place right off mix to make sure that we review things that goes out into the product. 
I whether that came from from AI or not or like a human that's that's kind of irrelevant, but tempting even the right piece of code. 

 
I can sit down and write, or I can make an AI write that code, right? 
But what matters is what the code is actually doing. 
So we are putting those checks to ensure what the code is doing is within the realm of what we want to be. 

 
Uh to whom responsibility of? 

 
Umm. 
A good question I. 
So are you familiar with what we're doing that has seek mentioned or the the part that we are doing specifically my team I am doing is very much in the like when you are when you analyze a bunch of data that's coming from the CT scanner for example, OK, I we try to make sure, OK, based on a lot of data that we have with previously trained data, uh with CT, we make certain decisions as to OK is this a like uh a like Dr is this like bone is this fat. 
So we can we can, we can meet certain decisions about the the composition of an image, right? 
So so that has no very little. 
I I don't know if that is subject to any any bias beyond like OK, if I mean yes, there's a like the the deep learning type of bias in the in a neural network bias. 
But that would kind of sway the the the answer. 
OK, we'll add more noise. 
So will not remove some of the noise that is in the image, but that's a very objective decision. 
There's hardly any opportunity for. 
Like uh, interpretation, because at the end of the day, the interpretation is actually done by the radiologist, right? 
So that's where the ethics and the subjective nature comes into the picture. 
And if we mischaracterize an image due to a problem in the AI, that's a bug. So. 

 
Ohh that's a good question. 
I think both. 
I think the company should take a good responsibility of what it puts out. 
Make sure that it it responsibly use the technology right? 
Uh, but also I think more responsibility, I would say come to the company, but the user has to also be aware of the potential biases. 

 
But on the other hand, we don't know how this will be used by different people, so that's where I would think that the company has a bigger responsibility to ensure that there will be not misused right? 
Then put the guards against those. 

 
Yeah. 

 
Right. 

 
Ah, that's an that's a very. 
OK, so I think we are mixing privacy and transparency now. 
I don't think that the reason like LinkedIn is not sharing. 
It's not necessarily because of privacy. 
It's probably they don't want their. 
Competitive advance stage to be to be lost right? 
So I think it's not because it's privacy, it's because they can share that details, be more transparent without I mean very similarly without like impacting adversely impacting the privacy of the users very similarly when when we have like a MRI scan or CT scanner for example, right, we share images, but we ensure that. 
With obviously with the the the uh like if for example, if there's if you want to investigate a a defect in a system with the permission of the hospital, we can get those images. 
Uh to our engineers to look at them, but we ensure that they are anonymized so that none of the patient's like privacy information, is not shared with us with the engineers. Right? 
So so we can get the data we can share without the privacy being affected. 
The LinkedIn can do the same, but I I don't think that they're doing it for privacy reasons. 
I think they're doing it for the competitive advantage. 
And yeah, I think in general, uh, it'll be a hopefully it'll be better. 
Better world if companies share their practices like think about Google searches, right? 
If I search for something right, what I get may not be exactly what you get when you do the search right. 
So now is. 

 
Is Google not sharing that because of a privacy privacy reason? 
No, I don't think so, right. 
So they don't wanna share their search algorithm, cause no, now everybody's doing right. 
 
Uh, not how it because. 
Not necessarily. 
How AI to be used? 
Because AI is a big umbrella, right? 
You have certain tools, so when certain tools are there, we have trainings for those tools that may be based on AI and certain practices like for example, how do you ethically. 
Ohm work with a companies like to make sure that we make decisions like a like healthcare providers. 
How do we interact with health care providers? 
How do we interact with researchers? 
Right. 
So so we have trainings for those now. 
Some of those make cover AI related information, but it's not necessarily because if you just say I, not everybody knows. 
Am I using here or not? 
They're not even aware of it, so it it's it's. 
It may have less impact if we just have, because then you have to kind of give examples of everywhere that yeah, is used and not everybody is using the the like in an equal way. 

 
 
