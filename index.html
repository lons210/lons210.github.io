<!DOCTYPE html>
<html>
<head>
    <title>Text Display</title>
</head>
<body>
    <p>
 Well, I think that it's a it's going to be a developing field because we don't really know where it's going to take us. artificial intelligence. Ethics, you know, would be the field of which you would study the problems, you know, intended and unintended the consequences from putting in trust in the intelligence outside of a human. Now, it's something we've done in the past. I guess you could say. We've done it with dogs. We've done it with horses. We've done it with a bunch of things. but we haven't ever done it with something that doesn't have what we would consider to be feelings. So 

 I think we're going to be surprised at times when the artificial intelligence does something that seems to us to be, completely. 

 Bonkers completely cruel, completely the void of any actual sense. 

 and that's what we see a lot of the Times with computer aided diagnosis. Things like those in radiology is that you'll see that it may suggest things for you to look at. You. Look at them. You say that's ridiculous. You know that that happens occasionally most of the time. It does a pretty good job. Sometimes it points out something. It's not even on the picture, you know something that's 

 that's completely away from what you would expect an intelligence to do. 

 having to think about the ethics is how much trust we put in this technology. How much? How do we control? It's scope. How do we control? 

 It's importance and how it affects humans. and and and how it affects things other than humans. I guess if you're really looking at the long term, 

 what scares me also is. 

how do we assign responsibility 

 for what the AI does? That's the thing that really comes into play. What I think about it as a medical professional 

 is that one of the main things that medical professionals do is accept responsibility and accept risk and accept. You know that you're the one that it's ultimately going to come down on. If you have an AI, for instance, we'll say we have an AI that does a great job of reading chest X-rays on a regular basis, or whatever, and it just flat out, misses a cancer that then goes on to kill someone 

 who's responsible for that is it the maker of the AI program? 

 Is it the hospital? Is it the doctor who's ordering the study? Who didn't look at him himself? Is it. Me? maybe functioning a supervisory. Ca, capacity for it. And are all of those steps going to be followed 

 in a responsible basis? If if the responsibility gets diffused among so many people that you can always assume the other guy is gonna do the right thing right. And that's where a lot of times. Responsibility in the field of medicine falls down. Is that you? You you you, you know. Let's say you have a cat scan that shows something. You dictate it. And okay, I've done my job. I've said that it's in here 

 But the physician maybe doesn't get the copy of the report. 

 So, and he's assuming well, if it was something I normally would have called me. And then the nurse is saying, Well, he's probably you know the the doctor that I'm working with is going to look at this on his own. You know, there's there's there's people problems. So the way we solve it currently is that everyone goes beyond what they think is their responsibility in order to make sure that important things don't get lost is AI gonna have the the capacity to know when something's truly important. And how is that? How are humans going to interact 

 with that? Are we gonna trust that it's always going to call us, because that's what it's supposed to do. Or are we going to be a little paranoid like we are with normal humans? I don't know. So it's it's an interesting field. I think that we can think about it, and I think we should think about it. And I think that once we think about it, we'll figure out that we didn't think about nearly everything. and then we'll have to think about it again and again and again. 

 yeah, so I guess. 

for for in what setting, I guess, for medical setting or for 

 yeah medical settings. So in a medical study, I think that whoever is making money off of the AI is responsible. 

 so If the hospital system is saving on money that, it would otherwise have to 

 a professional feed to physicians, and they're just relying on AI to do it. Well, it's their responsibility to make sure that it's not making mistakes, and that it's communicating its findings, and that there are no issues. If a radiologist undertake that responsibility, for instance, let's say they're they're reading. 

 you know. It's reading cat scans. Then it's gonna be the radiologist's responsibility to review all the images that are looked at and make sure that we agree with it. 

 Is that always going to stay the same? Absolutely not as AI gets better. I can certainly see that it's going to be less and less supervision, and and possibly at some point in the future, and I don't know how remote this will be, but I suspect, pretty remote. 

 that the I will be, it'll just be free ranging. And 

maybe you'll only be consulted when something happens. That's surprising. 

 But I think that the ultimate responsibility is. 

 And this is the. It's a sad way that medicine works is that the responsibility comes with people who are being paid to be responsible. 

 And so I don't think it's an okay dodge for the makers of the AI to say, well, yeah, we're selling the device. But we're not responsible for any of the professional use of this device. I don't think it's okay for the hospital. Say, well, yeah, we bought the device, but the makers of it. We're supposed to make sure it worked right. And I don't think it's okay for the doctors to say, well, sure, we referred test to that hospital, even though we knew it. A machine was reading it because we assumed everything was going to be right, and and everyone's got a hand in making money off of this transaction and making the patient, you know, taking care of the patients, but ultimately in, you know, paying for their kids. you know, school equipment and shoes and stuff. So it everyone's going to be a bit responsible. Who gets the ultimate responsibility? 

 I guess whoever signs off on the final report until a government agency says, Okay, now, these can be final reports without any human intervention, and then the government will be responsible. Which means that no one will be responsible. 

 Yeah, because we can do the Federal government. 

 It doesn't work. 

Oh, I think that it's a it's number one. I think it's gonna be absolutely necessary. But number 2, if you want to maintain a business with the liability that you're going to be assuming for, said device. 

 It, it's going to be something that you're gonna want to do. medicine is a very risk. Adverse business. If, because there is a lot of risk with medicine, I mean, people are living and dying based on how things go. 

 so it's very risky in Denver, and we all take as many steps as we can to mitigate that risk. A lot of the things that I do on a day to day basis 

 is on the off chance that this happens and that happens and the other happens. I still won't. You know the bad outcome won't happen So you're you're doing a lot of steps that are totally unnecessary in any other field, you know. You just do it the easiest way possible in in most fields in medicine. You do it in a way that 

 you don't think anyone could screw up, and then people still find a way to to surprise you and and and do it so with a with AI. If I I am afraid that probably there will be some places that don't 

 do that, and they will disappear because they'll be sued into oblivion. 

 And but I think that you're right, I mean, there's no way that that the the bureaucrats are going to be able to keep up with the ethics that we're going to evolve with with AI 

 They just. They they can barely keep up with telemedicine 

 and with computerized medicine. And you know, we can't even get a It'd be nice if we were able to share information across platforms. The problem with that is that everyone wants to write the program and they want to sell it. And as a free market economy. And all these programs. They don't talk together. And you have to protect the protect the information because of hipaa laws. And so you can't make everything jive together way it should. 

 Ideally, in a perfect world we'd have the best program. We choose it. We'd say, this is what we're using guys. And this is where all the labs are going to be, and we'd all be able to access it. And I know when someone was referred to me from, you know, 60 miles down the road, what their cat scan, and I'd be able to look at the images. 

 or I'd be able to look at their last labs, or I'd be able to look at their last doctors a visit. that's not the case, and so we can't even make that happen now a a totally 

 non-human intelligence, making decisions that you really don't know what's in the black box, because it's proprietary. No one's going to share their AI with anyone. That's how they make money, and so we won't know how the decisions are made. 

 or we won't know how 

 errors that you incur you. You you experience as a physician how you learn from we don't know how the machine is learning those things, either. It's it's going to be an interesting situation, and you're, I suspect, a lot of it' be like, Oh, yeah, it's not very good at doing X, or and you know, so we've got to kind of make sure that it's okay. 

 that's that's something that's gonna happen as a human oversight. But that being said, physicians are not computer science people that work with AI all the time. And so we're only as good as 

 It will be limited as far as what we can do. 

 I don't know if I answered your original question. I kind of went to Field. 

 that's a great question. I mean, can you create an ethical human. 

  I'm not. I'm not entirely convinced of that yet. you know. What you can create, I think is something that simulates an ethical machine. 

 So if you were to have a Turing test, and you were to have a computer in another room, and you were talking to it, and you thought it was a a a kind 14 year old girl. I'm pretty sure that we could probably simulate that with an AI right now, and you might not know the difference. Now, do I think that machine actually cares? No Do I think that it could simulate the appearance of caring? 

 Yes, and is there a difference between those 2 things? that's where you get kind of the gray area for me. I'm I'm not really entirely sure that that that you can. 

 I try to be ethical. But am I trying to be ethical, or am I trying to make decisions that an ethical person would make. I'm not sure I'm not sure how to be for the AI. But I assume it'll be one step removed from 

 for me, because at least I'll feel bad. The AI. 

 It'll put it in its algorithm and say, Well, that didn't work. Let's try something else next time, you know. And and 

 it'll, I have no doubt, some sort of really impressive array of information that it has, and it'll select a an appropriate choice from 

 one of those thousands of responses and see how that one works. 

 I don't know it. It it I'm not sure we're at that level of complexity, but 

 I think that when we get to that level of complexity that 

 AI stands a chance of blowing right past that level of complexity and going to a level of complexity that none of us actually understand 

 or can ethically control. 

 that. That's where I think that's where I think the term singularity comes in. You know, you're worried about. Okay, let's get this thing as smart as it can be. 

 What happens when it's so smart that we're not really sure why, it's making its decisions that that we didn't. We weren't able to figure out ourselves. It? It'd be very interesting. 

 I think we'll get there eventually. 

They just know that it does. 

 it's it's a funny thing. I mean, yeah, I I think it's a it'll be an interesting tool if we can use it effectively. And and it's okay with being used effectively. 

 So what do you do? As the most beneficial and the most detrimental parts of an AI 

 So the AI's got access immediate and always on perfectly memorized access to all information. 

 so if it wants to know, you know, what are the different 

 types of drugs in this class of medication. It knows everyone. It knows the molecular formula of it. It knows when it was made. It knows how it works percentage-wise on whatever you know, you're treating So just the amount of information, the collating of the information, and the ability to access it and apply it 

 rapidly is, you know, far exceeds. We were talking about this the other day about how. 

 oncologists, for instance, they have to read all the time to keep up with the amount of medication and the trials and the the 

 you know, they're starting to use things that are bioactive and specifically designed for certain tumors and certain genetic types. that would be the kind of thing that if an AI was able to do it could just spit out a two-page treatment protocol that would have the best chance of success, whereas. 

 you know, some of the top on colleges might be able to do that. But I guarantee you not every oncologist could could do that for sure, and they could do it effortlessly for 

 all day long, every day. 

 Whereas a a human only has certain amounts of time that they can think intensely and make right decisions. 

 Yeah, so that that's the strength of it. They're tireless. They know everything. 

 They may not be able to reason it out as well as we can get. but they have it available. And they. if it's a type of decision that can be made based solely on statistics. 

I think they've got the edge every time. 

  

 yeah. So the downsides  would be 

 the unexpected cases, whereas a human might not even realize that they're running into an unexpected situation. They would. You just be so obvious to us that we would just, you know, immediately. Go to the right choice for an AI. I think they could get fooled 

 just from lack of a discrimination. 

 No, I don't think they'd make that if you told it. Hey? That was the wrong thing you did. Don't do that again. I don't think they'd make that same mistake. but a new mistake. Even minor differences in that mistake 

 they might just make again and again. I'm not sure 

 so it it's really, I think the AI is only going to be as good as the programming and only as good as its database. And so 

 yeah, it. It'll be interesting to see if people will will work together. Different companies will work together to feed 

 databases that will be effective in providing care. 

 Would you feel comfortable with. 

 So these are employees that I would have personal responsibility for. 

 yeah, I mean, I guess within reason. So let's say someone had an infection, and we were trying to figure out what antibiotic is going to be most effective. 

 And if they did put it in the AI. And the AI said of these 2 medications, this one is, you know, 85% likely to help. And this one is 30. I think. Yeah, I'd love to have that my nurse come to me and say, well, this is the one that's recommended. We already do that kind of testing with susceptibility, testing with antibiotics and with organisms. 

 that's one step removed from just telling. Just asking the AI. Hey? This is better. Maybe it's not even one step removed. Maybe it's exactly the same thing. I don't question that 

 so I don't know why I would question the other. 

so would you be 

 Oh, yeah, absolutely. You'd want 

 if you had a question about it. If the AI was able to to explain itself and to 

 a and in a concise fashion, a a rational, concise fashion that a human can understand. That would be much better. because then you could feel confident in the decision making 

 and and you could learn something you'd become maybe a better doctor, or you would be able to correct 

 a mistake that it's making. you know, one of those obvious mistakes, a human that so that there's gonna have to be 

 good interfaces between humans and machines. And then the question becomes, well, how do you weed out 

 good interactions and true data from bad interactions and false data. 

 that people are providing to the machine if it if there is sort of a 

 2 way learning street between physicians or health personnel and the AI and the computer programmers. 

 you know, like what what won't happen, what won't work is, if I call, I pick up the phone and say, Hey, you're a I made this mistake, and this is what I think needs to happen. I mean. Obviously, there's so many steps in there. 

 It's like the game of telephone it it won't work. 

 but I can see that happening. 

 We have to already do that with a, you know, computerized health records, we have to call someone up in St. Louis and hey, the machines not working. And okay, we'll get around to it on next Tuesday, and it's it. So I I assume it' be the same thing with the AI. I can see maybe there'd be something where you'd say. If you run into this kind of problem, be sure to flag it for me, and 

 I'll look at it. And as a human decide whether or not you're right. 

 that that would be a good interim step. I guess. I think all these things we're talking about are interim steps. 

 You know. I I don't think we decide should be a this is, for sure, the only thing we're ever going to do, and this is. 

 you know, as far as it can go. Obviously, we're going to have to do incremental 

 steps. And we're gonna have to step back. And we're gonna have to say is what we're doing. Ethical. Does it make sense? Is it helping? People? Are people, you know, surviving longer, or have some sort of outcome that shows that you're helping. Maybe the cost of care is less, but the 

 quality of care seems to be higher. It' be. It'll be an interesting challenge. Yeah, I guess, maybe, being a statistician that programs computers to do statistics will be a great field. So you can prove that what you're doing is actually effective. And then we'll have to rely on them in order to tell us whether what they're getting paid to do is good. So maybe that's not such a great idea 

 to be fair. Accountants have also created their own profession, based on creating a system that only it's it's exactly the same thing, you know, and I'm sure that a high end tax attorneys and accountants 

 live in a realm that we don't really recognize as real as as regular people. But so, yeah, to be fair, I mean, exactly. So we have to try to be careful not to be freaked out, because it's not a human. But we have to realize this thing in a human. It's not gonna 

 have the same ethical considerations that we have or concerns. Perhaps, definitely. 

 Yeah. So what type of AI, do you use any in your organization currently? Or do you use any personally? 

 so I use very little. AI, unless you're talking about? 

 well, I I suppose I use a lot of AI. But you talk about like Google searches. I'm sure that that could be considered straight up. And AI 

  does it tell me the answer? No. But do I ask it to convert? 93? So it's the Fahrenheit to make it easier on me. 

 Yeah. Did he get the answer that I kind of thought it was going to get? Yes. 

 but it's like a good double check in work itself. We use computer aided diagnostic for for mammography. 

 which a lot of my partners read. And what it'll do is it'll go through images, mammographic images. And it'll flag areas 

 that it thinks that we should look at closely. 

 So areas of micro calcification or areas of architectural distortions, things that aren't quite right. And you may be that you look at it. And you say, that's that's nothing. Or, Oh, yeah, that that is something we need to take another picture of that. So that's helpful in sort of jogging the humans elbow like, Hey, did you look at this? You know? 

 yeah, so that that that exactly this could be important. This could be something to do. that's about the extent of it right now 

 the field that I do. Interventional radiology. I'm working a lot with obviously with computers and with cat scanners and and florowski machines. So a lot of technology, not a lot of AI per se. 

 no. 

 But I mean, I guess the real question is, would I notice a increase in people using it in my field, I would expect that people advertising for cosmetic services 

 are not using real models at this point. 

 I mean, if you wanted a petite, brown haired lady 5, 6, with great complexion. 

 Why wouldn't you put that into the chat bot? Have it? Generate an artificial person, not pay an image company to generate something and then be able to use this particular person for all your advertising needs. 

 I mean, it's it's pretty obvious. So from an advertising perspective for medicine it certainly could be I I can see how you would want to have it. 

 it just so. I don't know. I mean, would we notice it. I'm not sure we would. 

  and I don't think people, I think maybe maybe that's a great question. 

 Should those kind of things have to be ethically disclosed. 

 This is not a real patient. This is a computer simulation. 

 you know. You're not necessarily. You could have the person have it before and after. and get great cosmetic results right? And that would be so so very easy to do 

 is that that's that would have to be ethically disclosed. And is that going to be something that we can control? I don't think so. Not right now. 

 right now we use a for for deciding whether or not to do studies we use, obviously a kind of hospital research committee, Irc's. And and in order to try to be ethical about things, 

 I I would think that you would have to have something along those lines. 

 but knowing I don't know if you'd include 

 your it, guys in that or if you would just have the medical professionals trying to decide what's ethical. 

  and then overlooking things and then going back and fixing it, you know, I think you'd have to have the 

 maybe the It people involved. But again, you know, it is not not necessarily A AI experts. I think this is a fairly rarefied group of people. 

 maybe they need. Maybe we need to have a society where you could hire someone to come in and ethically talk about how you're going to utilize AI in your hospital practice or in your business practice? 

 maybe that I mean because you have to have someone that understands the the foiables of the AI, but understands the capabilities and understands where it's going. 

 All those things are things that we, as medical professionals, would not probably have a full grasp upon, we would have a good grasp. We knew it was someone unethically doing surgeries, or, you know, doing things that we do I'm not sure we'd be as quick to pick up on the fact that something we're doing with the computer has a potential of being hacked by, you know, or sold across the country to someone else. that would be hard to know 

 what if someone's selling you AI to use in your computer system, but that AI has a back door that allows patient information to be transmitted to other places. 

  that would obviously be unethical. Where do we be able to detect it? I'm not sure. 

 Would we be able to trust the computer, the companies that make these a I doubt it. 

 if there's millions and billions of dollars involved. How do you do that successfully? without totally cutting yourself off from 

 the Internet? 

 I don't know. It's a good question it I haven't really thought of it, but 

 you would assume anything that's smart enough to think at the levels that we're talking about. 

 It's probably pretty easy to hide some code in there that occasionally sends a blip of information across the Internet. 

 so? What? So I think 

 you do run the risk of. So we talked a little bit about who's going to ultimately take a responsibility. 

 And so there are doctors that pursue that with, with. you know, the kind of approach that you would expect. that you'd hope for the the that I'm going to take responsibility, and I'm going to know what's going on. And my nurse practitioners. I'm going to monitor them closely, and we're gonna 

 come to the best outcomes for our patients, and then you have, conversely, you also have positions that maybe run 15 nurse practitioners. and they're really monitoring them only in name. 

 Is that ethical? 

 No, I don't think so. medical board something. So is it legal? Yeah. 

 So 

 I think you run the same risk with AI, let's say 

 I want to read, you know. 

 2,000 ct. A day 

obviously as a human, I can't do that. Maybe I can read 40 or 50, or 60, or whatever it is. 

 But if I want to increase my numbers by 15 times. 

 and I make sort of a cursory effort to look at each of these things. is that fulfilling my ethical responsibility? No. But is it obviously financially lucrative? And is someone going to do it absolutely. Someone doesn't do it without a doubt. 

 So there, there's there's a thing you're you're making the ability to fastly increase your your ability to do work without perhaps accepting the personal responsibility for that work. 

 And that's, I think that's one of the main problems with the AI yeah, using it in a responsible way. Again, it's it's all comes down to personal responsibility. Now the other thing would be is that if you are trying to use it responsibly. 

 Are you technically sophisticated enough 

 to do that? That's the other thing. Again, we're not computer scientists. We're we're physicians and medical professionals. And 

 so it's going to be a a to our understanding of what can and cannot be accurately done. And there's going to be some hiccups in the beginning for sure. 

 when we start using it more and more. 

 Well, sure, if anyone had access to a doctor level AI Then they would have the idea that 

 that they could diagnose and potentially treat whatever they needed to treat. And maybe in most cases they'd be right. 

 in some cases they wouldn't. there's 

 but with it matter. that's a good question. I mean, if I if my secretary decided that she wanted to be a 

an oncologist 

 and and recommend stuff to her mom. Well, it still wouldn't matter at this point, because she couldn't prescribe the medications that were needed 

 unless she stole someone's credentials and and did so. 

  So so 

 if you, there are physicians, however, that will allow people to use their credentials to put information in maybe lab results and stuff like that. Obviously, that's that's round upon. if not outright forbidden. But you know, I'm sure it happens. 

 I don't do it, but I'm sure people do 

 Could that work out for the wrong? Possibly  as far as using it 

 unethically. Possibly you could. 

 The other thing you would think about is, it's kind of a hipaa violation. What if I knew that the 

 the certain set of lab results. And that person was like, well, I'm kind of curious as to what my neighbor, you know, who came into the doctor's office. 

 What what's going on with them, and they put it in the AI, and they find out they've got lymphoma or something that that could be potentially a hip of violation. so they, you know there's there's some power there. 

 Does that really hurt anyone? I don't know. Maybe. 

 man, I don't tell everyone any medical stuff I have. 

 So I would assume other people would want to keep things private as well. 

  as far as my personal employees. I think, as long as you've maintained the desire to know about important decisions they will 

 be all too happy to share those kind of decisions with you. 

 and behave in a in an ethical manner. And so, if it were those kind of people like my nurse practitioners my head text. 

 they would, I I totally trust them to do the AI, and then tell me, hey, this is what I found when I put it in there and 

  vision. Yeah. 

 But is there any sort of a 

It's hard to get 

 medical professionals and doctors to go anywhere to do anything with time intensive. 

 I mean, I'm going to be honest. 

 so like requiring a medical professional to go and spend 30 h learning how to use AI, 

 that they view is possibly something that's gonna take away their job. 

 It's gonna be, you know. 

 little challenging. 

 would we? Now do. We do lots of training for other things that are new, new advances in our field? Absolutely. so maybe that would be the approach. I suspect that the less technical. 

 you are. So I mean, you're talking to to a profession that's pretty far along the technological 

  medicine tree 

 with radiology. And we're computers all day long every day. 

 We would probably accept it better 

 and be more willing to apply it. I suspect that a 

 60 year old doctor that's been making diagnoses all his life, just by talking to people and listening to their backs and stuff. might be less inclined to want to spend time doing that.  from the ethical standpoint, you know. I they would to be less inclined to use it, and they would be  It'd be hard to convince them to sit in on an ethics about it, too, I would think. 

 but I think you could do it. I I guess you'd have to make it a requirement right now. They may have a requirement for us to do a certain amount of opioid prescribing courses which everyone thinks it's stupid, but we still have to do. It's not, it's it's only stupid. And let me preface this. it's only stupid, because when I was going through mess medical School. They pushed the exact opposite. 

 No one should have pain. You've got to prescribe it adequate opioids, and we all kind of thought of the time, Jason. It seems like a lot of really hard drug to be pushing all the time. It seems like a lot to do. Guys, yeah, are you sure we ought to be doing that? And then, now, 20 years later, like. you guys have been prescribing too many opioids. You need to. This is what you need. You told us to you crazy people. 

 Yeah. So that's that's the way that's where health care administration works. 

 So I don't know. I I, for one, welcome our robotic overlords. 

 so it'll I'd be willing to. 

 Well, no, I if it makes it more efficient and makes care better. That's better for everyone, less liability. 

 less potential problems. 

 if we have the papers and the data that back that up. 

 you know. But we have to have. We have to have the information that it's not only 

 better for us, but also better for the patient. 

 I think. So. I think so. 

 I think there's a lot of talk about what AI could do. There's a lot of talk about how people are going to lose their jobs, and there's no talk about who's going to take the responsibility. 

  which I find to be pretty funny, because. 

 the only thing medicine is is taking responsibility. that's like, that's why we get paid money and 

 it. But it. But you hear, oh, yeah, you know, radiology. You're not going to have a job in 10 years, or whatever that. We're not terribly concerned about that, because it's obviously haven't thought through these issues completely. 

 you know. And so it'd be an absolute 

 disaster should just be released without any thought 

  towards these things. But I guess they might. 

  I'm a little bit insulated in that. We haven't created robots that will be moving around and sticking people with needles and handling catheters, and you know, so I mean, I don't think they can replace me for 

 quite a while because of that, too. 

 They'll lose their house if someone dies. 

 Well, I mean those 3 jobs that you created to make a super 

 family practice, doctor. 

 if someone has rip or and diabetes and goes into keto acidosis and ends up dying in the er because the AI didn't diagnose it 

 is the person who made the I gonna lose their, their, their, their home 

  and all the resources? I think not. So that's the thing is that sure jobs have been created. Are they the same level jobs? 

 I don't know, because they don't have the same level of liability. 

 I guess, to make that simplifying it. I mean, jobs are jobs, and any job that you do well, as a job worth doing. But I never felt the degree of personal responsibility when I was working for a moving company calling a 

 sofa that I do when I'm sticking someone's liver for a tumor. You know what I mean. It's it's more personal, and it's more clear 

 where the responsibility will lie. Should anything happen. So that's where I I'm afraid of. If we diffuse the responsibility, then no one's going to be responsible. 

 Patients aren't going to have any recourse as to when things go wrong. 

 there won't be any 

 sort of damically hanging over anyone's head to fix the problem because they want to, you know, make the numbers for the next quarter. Right? 

 And so there's a profit. 

 There's definitely a profit component involved in this and the proper component will not consider 

 liability. because, though. they'll be out in in the Bahamas before this all crashes down right? 

 It's good. 

  possibly right, anyways. 

 negative way to look at it. I guess 

 an interesting thing that I think you could think about is, what about if this becomes like ch chat? you know, AI, where now you can go online. And possibly there is already this who knows? where you can put in your symptoms, and you can have a diagnosis. 

 get it printed out, and then go to the doctor and start quizzing them about whether it could be this disease or that disease or this other rare entity. I need this genetic testing. 

 Who's you know? How is that going to play out in medicine? That that would be a very interesting thing where you kind of put the power and the information in the hands of people that really don't know what to do with it. 

 and then start demanding 

  things. So that would be a thing that could really drive up the cost of medicine pretty dramatically, because if someone comes in and they say I've got these 10 symptoms of this rare genetic disorder. And I want you to send off for molecular testing that costs thousands of dollars. 

 How are you? Gonna say, no. I'm not really. That's going to be a hard thing. 

 yeah, I don't think so. And then they have it. Then you're like, oh. 

  you know, or they don't have it. 99% of the time, is it a you know, it's a we try to judge things by years of life gained per cost, or, you know, the quality of life gained for costs. Those kind of things. That's a a a standard way of measuring 

 whether you're doing good or not in medicine. so that that's a definite thing that you might really lower that one of the things they talked about a lot also is in in medical school is how we spend such a large percentage of the dollars in the last year of a person's life. 

 it's like 90% or something in the last year of life. is, that 

 is, you know, that's obviously not ideal. But the re, a lot of that is covering 

 liability. Obviously that wouldn't be good if we get rid of liability with the AI boy. That would be a win win for everyone. Maybe 

 you know what I mean, because then in the er doctor, where where you know a a 

 a 20 year old falls down, and you know, bumps their head wouldn't feel obligated to get a cat scared. 

 No, this is only once every 500,000 times. This is, you know, no way. 

 Then you could say, well, you know 

 I I was justified not doing so. I don't know 

 just a thought. Okay, I don't. I hope I was at least marginally helpful. I feel like, maybe I'm 

 bringing up more problems than I am solutions, I mean, I think that AI stands like I said, it's hard to think about 

 how disruptive it could be it it I think it would be, could be absolutely amazing. 

 in a in a good way. 

 the problem is is that I don't know that we're gonna have the intelligence and the foresight to use it amazingly. Yeah, you know. So I think ethical questions are. 

 I think they're fantastic. and I think they need to range, you know. 

 Are we protecting people's privacy? Are we improving their quality of life? Are we improving their lives in general, we are we doing things to harm other people? 

 Are we taking responsibility for things that we need to improve on a daily basis. Those are all all really important ethical questions that you know you probably ought to face 

 in your own life doing anything but it. It'll be interesting to see how we apply that with. AI, yeah. 

   but is I don't know. I don't think we probably I I don't know that we do. I mean. 

  for instance, when we got a new mammography unit with A 

  with. Then it has some AI built in, you know, like we talked about computer aided diagnosis. 

  I I think the review process was. 

 send us over a quote of how much it'll be. The radiologists give their input. About which system they'd like the most. 

  I don't know that there was a lot of talk about, hey? Our computerator device 

 it, you know, text 95 of the abnormalities. And there is only does 90. I 

 I don't know. So I would expect that there's probably an Ethics Committee. and they would probably be well over their heads with dealing with something like this.  And so I would think that we would probably be in, you know, and I'm in a fairly small 

  city with community hospitals. I think we'd probably be in a wait and see kind of a a mode for We one of the bigger hospitals 

 to start doing that, and then presumably to to kind of do outreach from there 

  as to how to implement it safely. 

 and that's the way it probably ought to be. you know, started it. 

 John Hopkins and Harvard and Emory, and wherever you know these these bigger places. 

 and then 

 find out some of the problems of places, have money to solve problems, and then and then start kind of trickling down to other places 

 would probably be the best way to go 

 yeah, other than just straight up. Boom! I got this off the Internet. I think it's going to be great. 

 What could go wrong? We'll see. No, it would be cool, you know it'd be nice for my own practice. What would be cool is if we dictated something. 

  This is the way I would do. This is, I would look at a cats. Can I dictate it? And then I'd put it, I'd I'd have the AI look at it 

and see what it reported. 

and then, and that would be the same thing. I'd I'd look through what it said, and I would say, oh, I the same way I use We have preliminary reads overnight. so I'll look at the study. I'll dictate what I think, and then I'll look at the preliminary read. 

and I'll see if they said anything that I didn't say. 

and then I'll look at, I'll say, no, I think they're crazy. That's not right, or I think, oh, yeah, I guess. Yeah, that does that. That's a good point. Yeah, exactly. That's the way I would use AI in my own practices that I would try to use it, as it's sort of a double blinded, you know. I'm going to read it, and then I'm going to look at what they said and then compare it. 

  

I kind of like it. The second opinion. But just for you opinion exactly. It. Just a second opinion with everything you look at 

that'd be that'd be the best way to do it in radiology, I think. 
</p>
</body>
</html>
